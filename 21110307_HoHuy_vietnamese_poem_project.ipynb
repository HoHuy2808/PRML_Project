{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7490456,"sourceType":"datasetVersion","datasetId":4361058}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-29T16:22:15.100033Z","iopub.execute_input":"2024-01-29T16:22:15.100599Z","iopub.status.idle":"2024-01-29T16:22:16.896046Z","shell.execute_reply.started":"2024-01-29T16:22:15.100572Z","shell.execute_reply":"2024-01-29T16:22:16.895072Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/vietnamese-poem-dataset/poems_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:22:21.308584Z","iopub.execute_input":"2024-01-29T16:22:21.309418Z","iopub.status.idle":"2024-01-29T16:22:21.313569Z","shell.execute_reply.started":"2024-01-29T16:22:21.309385Z","shell.execute_reply":"2024-01-29T16:22:21.312652Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade pip\n!pip install -q transformers huggingface_hub wandb\n!apt-get install git-lfs","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:22:23.927848Z","iopub.execute_input":"2024-01-29T16:22:23.928740Z","iopub.status.idle":"2024-01-29T16:22:41.430262Z","shell.execute_reply.started":"2024-01-29T16:22:23.928705Z","shell.execute_reply":"2024-01-29T16:22:41.429258Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit-lfs is already the newest version (3.0.2-1ubuntu0.2).\n0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:23:22.236440Z","iopub.execute_input":"2024-01-29T16:23:22.236941Z","iopub.status.idle":"2024-01-29T16:23:22.567120Z","shell.execute_reply.started":"2024-01-29T16:23:22.236896Z","shell.execute_reply":"2024-01-29T16:23:22.566134Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ffbdfc07ec47438d4c1292cd18cef4"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\n\n# Load the CSV file into a DataFrame\ndata = pd.read_csv('/kaggle/input/vietnamese-poem-dataset/poems_dataset.csv')\n\ndef build_text_files(data_df, dest_path):\n    f = open(dest_path, 'w')\n    data = ''\n    for index, row in data_df.iterrows():\n        summary = str(row['content']).strip()\n        summary = re.sub(r\"\\s\", \" \", summary)\n        data += summary + \"  \"\n    f.write(data)\n\n# Split the data into training and test sets\ntrain, test = train_test_split(data, test_size=0.15)\n\n# Write the training set to a text file\nbuild_text_files(train, 'train_dataset.txt')\n\n# Write the test set to a text file\nbuild_text_files(test, 'test_dataset.txt')\n\nprint(\"Train dataset length: \" + str(len(train)))\nprint(\"Test dataset length: \" + str(len(test)))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:23:41.407386Z","iopub.execute_input":"2024-01-29T16:23:41.408177Z","iopub.status.idle":"2024-01-29T16:24:05.607133Z","shell.execute_reply.started":"2024-01-29T16:23:41.408142Z","shell.execute_reply":"2024-01-29T16:24:05.606073Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Train dataset length: 145504\nTest dataset length: 25678\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model + Tokenizer\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"danghuy1999/gpt2-viwiki\")\nmodel = AutoModelForCausalLM.from_pretrained(\"danghuy1999/gpt2-viwiki\")\n\ntrain_path = 'poem_train.txt'\ntest_path = 'poem_test.txt'","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:24:08.187457Z","iopub.execute_input":"2024-01-29T16:24:08.188285Z","iopub.status.idle":"2024-01-29T16:24:21.204154Z","shell.execute_reply.started":"2024-01-29T16:24:08.188252Z","shell.execute_reply":"2024-01-29T16:24:21.203388Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/916 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d921b531e241c6afe85cb4e818b06d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"109ad455398443a8b2ceb53f131093a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/431k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a0a01a7978143c2bc84af0971360358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a84c2aa83374ff784a686921f6b1178"}},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.encode(\"<|startoftext|>\")) # fail to encode this token\nprint(tokenizer.encode(\"<|endoftext|>\"))\nprint(tokenizer.encode(\"\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:24:52.947999Z","iopub.execute_input":"2024-01-29T16:24:52.948936Z","iopub.status.idle":"2024-01-29T16:24:52.965643Z","shell.execute_reply.started":"2024-01-29T16:24:52.948897Z","shell.execute_reply":"2024-01-29T16:24:52.964481Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[28, 92, 1472, 1632, 1247, 19862, 92, 30]\n[0]\n[]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(tokenizer))\ntokenizer.add_tokens([\"\\n\"])\nprint(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:24:57.186089Z","iopub.execute_input":"2024-01-29T16:24:57.186889Z","iopub.status.idle":"2024-01-29T16:24:57.208503Z","shell.execute_reply.started":"2024-01-29T16:24:57.186845Z","shell.execute_reply":"2024-01-29T16:24:57.207529Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"50257\n50258\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:24:59.435661Z","iopub.execute_input":"2024-01-29T16:24:59.436022Z","iopub.status.idle":"2024-01-29T16:24:59.443221Z","shell.execute_reply.started":"2024-01-29T16:24:59.435995Z","shell.execute_reply":"2024-01-29T16:24:59.442330Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"GPT2TokenizerFast(name_or_path='danghuy1999/gpt2-viwiki', vocab_size=50257, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50257: AddedToken(\"\n\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Resize model's word embedding","metadata":{}},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))\n\n# New weight for our new tokens (all zeros)\nwith torch.no_grad():\n    model.transformer.wte.weight[-1, :] = torch.zeros([768])\n\nprint(model.transformer.wte.weight.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:25:01.926169Z","iopub.execute_input":"2024-01-29T16:25:01.926569Z","iopub.status.idle":"2024-01-29T16:25:02.530720Z","shell.execute_reply.started":"2024-01-29T16:25:01.926526Z","shell.execute_reply":"2024-01-29T16:25:02.529710Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([50258, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TextDataset,DataCollatorForLanguageModeling\n\ndef load_dataset(train_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=100)\n     \n    test_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=test_path,\n          block_size=100)   \n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,test_dataset,data_collator\ntrain_path = '/kaggle/working/train_dataset.txt'\ntest_path =  '/kaggle/working/test_dataset.txt'\ntrain_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:25:11.331185Z","iopub.execute_input":"2024-01-29T16:25:11.331909Z","iopub.status.idle":"2024-01-29T16:27:39.027955Z","shell.execute_reply.started":"2024-01-29T16:25:11.331877Z","shell.execute_reply":"2024-01-29T16:27:39.027100Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialize Trainer with TrainingArguments and GPT-2 model","metadata":{}},{"cell_type":"code","source":"%env WANDB_PROJECT=GPT2-POEM\n%env WANDB_WATCH=all","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:28:04.341721Z","iopub.execute_input":"2024-01-29T16:28:04.342707Z","iopub.status.idle":"2024-01-29T16:28:04.348400Z","shell.execute_reply.started":"2024-01-29T16:28:04.342674Z","shell.execute_reply":"2024-01-29T16:28:04.347436Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"env: WANDB_PROJECT=GPT2-POEM\nenv: WANDB_WATCH=all\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/GPT2_Poet')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:28:06.846480Z","iopub.execute_input":"2024-01-29T16:28:06.847357Z","iopub.status.idle":"2024-01-29T16:28:06.853182Z","shell.execute_reply.started":"2024-01-29T16:28:06.847325Z","shell.execute_reply":"2024-01-29T16:28:06.850879Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom transformers import TrainerCallback\nfrom transformers import Trainer, TrainingArguments,AutoModelWithLMHead,EarlyStoppingCallback\n\nclass StopTrainingCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step > 1500:\n            control.should_training_stop = True\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/GPT2_Poet\",\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs= 5,\n    per_device_train_batch_size= 32,\n    per_device_eval_batch_size= 32,\n    evaluation_strategy = 'steps',\n    eval_steps = 500, # Number of update steps between two evaluations.\n    save_strategy = 'steps',\n    push_to_hub=True,\n    hub_model_id = \"GPT2_Poet\",\n    save_total_limit = 10,\n    warmup_steps = 1000,\n    report_to=                      'wandb',\n    run_name=                       'Run 6 - w/o label smoothing',\n    logging_steps =                 5,                    \n    gradient_accumulation_steps=    2,\n    learning_rate=                  5e-4,\n    weight_decay =                  0.5,\n    dataloader_num_workers = 2,\n    # label_smoothing_factor = 0.3,\n    load_best_model_at_end = True,\n    metric_for_best_model = 'eval_loss',\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience= 5),StopTrainingCallback()]\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:28:09.104686Z","iopub.execute_input":"2024-01-29T16:28:09.105039Z","iopub.status.idle":"2024-01-29T16:28:11.166414Z","shell.execute_reply.started":"2024-01-29T16:28:09.105011Z","shell.execute_reply":"2024-01-29T16:28:11.165629Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:28:14.629876Z","iopub.execute_input":"2024-01-29T16:28:14.630623Z","iopub.status.idle":"2024-01-29T17:34:27.726060Z","shell.execute_reply.started":"2024-01-29T16:28:14.630586Z","shell.execute_reply":"2024-01-29T17:34:27.724819Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240129_162828-46oc0aqb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hohuy-work/GPT2-POEM/runs/46oc0aqb' target=\"_blank\">Run 6 - w/o label smoothing</a></strong> to <a href='https://wandb.ai/hohuy-work/GPT2-POEM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hohuy-work/GPT2-POEM' target=\"_blank\">https://wandb.ai/hohuy-work/GPT2-POEM</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hohuy-work/GPT2-POEM/runs/46oc0aqb' target=\"_blank\">https://wandb.ai/hohuy-work/GPT2-POEM/runs/46oc0aqb</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1501' max='6405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1501/6405 1:05:11 < 3:33:17, 0.38 it/s, Epoch 1/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>5.660500</td>\n      <td>5.524101</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>5.107700</td>\n      <td>5.046876</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>4.841400</td>\n      <td>4.758120</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1501, training_loss=5.545836874995527, metrics={'train_runtime': 3961.1211, 'train_samples_per_second': 207.044, 'train_steps_per_second': 1.617, 'total_flos': 9804626265600000.0, 'train_loss': 5.545836874995527, 'epoch': 1.17})"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub('GPT2_Poet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.push_to_hub('GPT2_Poet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the model","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T17:34:57.270860Z","iopub.execute_input":"2024-01-29T17:34:57.271240Z","iopub.status.idle":"2024-01-29T17:34:57.278151Z","shell.execute_reply.started":"2024-01-29T17:34:57.271204Z","shell.execute_reply":"2024-01-29T17:34:57.277031Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained(\"tuanle/GPT2_Poet\")\nmodel = AutoModelForCausalLM.from_pretrained(\"tuanle/GPT2_Poet\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T17:35:18.406218Z","iopub.execute_input":"2024-01-29T17:35:18.406960Z","iopub.status.idle":"2024-01-29T17:35:23.029098Z","shell.execute_reply.started":"2024-01-29T17:35:18.406929Z","shell.execute_reply":"2024-01-29T17:35:23.027991Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/589 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ff33bfeccf4744aca4cb8f11c3158d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ecfbf599e84cdbb9b36da6e2c599a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/431k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ac889205e642cbae9bdda288631392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c2a5995cee4057a9ec9a6bf30d9757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/13.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a539801ac6e14812ad93ded7847792d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a71cb68cf84088a2f1493c6d659e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f0c18777ae4fe88c689c44cc49d359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0558a2926e044626b223bdb0a730b67d"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"hôm nay\"\n\ninput_ids = tokenizer.encode(text, return_tensors='pt').to(device)\nmin_length = 60\nmax_length = 100\n\nsample_outputs = model.generate(input_ids,pad_token_id=tokenizer.eos_token_id,\n                                   do_sample=True,\n                                   max_length=max_length,\n                                   min_length=min_length,\n                                   top_p = 0.8,\n                                   num_beams= 10,\n                                   no_repeat_ngram_size= 2,\n                                   num_return_sequences= 3)\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\">> Generated text {}\\n\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))\n    print('\\n---')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T17:38:00.300095Z","iopub.execute_input":"2024-01-29T17:38:00.300456Z","iopub.status.idle":"2024-01-29T17:38:02.431029Z","shell.execute_reply.started":"2024-01-29T17:38:00.300428Z","shell.execute_reply":"2024-01-29T17:38:02.429862Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":">> Generated text 1\n\nhôm nay trời đã sang đông\nnhớ mùa thu đến bên cầu đợi chờ\nđêm thu hoa nở bên đường\nmênh mông sóng vỗ mênh mông cõi lòng\nvần thơ lục bát thẫn thờ\nmà nghe tiếng sáo vi vu lời ca\nhỏi rằng ai ở đâu đây\nthưa rằng ta ở đây đây đâu mà\nmột mình một bóng trăng vàng\nngười ta gặp gỡ một mình gặp nhau\ncớ sao gặp lại một người\nđể cho ai lại gặp\n\n---\n>> Generated text 2\n\nhôm nay trời đã sang đông\nnhớ mùa thu đến bên cầu đợi chờ\nđêm thu hoa nở bên đường\nmênh mông sóng vỗ mênh mông cõi lòng\nvần thơ lục bát thẫn thờ\nmà nghe tiếng sáo vi vu lời ca\nhỏi rằng ai ở đâu đây\nthưa rằng ta ở đây đây đâu mà\nmột mình một bóng trăng vàng\nngười ta gặp gỡ một mình gặp nhau\ncớ sao gặp lại một người\nđể cho mình lại gặp\n\n---\n>> Generated text 3\n\nhôm nay trời đã sang đông\nnhớ mùa thu đến bên cầu đợi chờ\nđêm thu hoa nở bên đường\nmênh mông sóng vỗ mênh mông cõi lòng\nvần thơ lục bát thẫn thờ\nmà nghe tiếng sáo vi vu lời ca\nhỏi rằng ai ở đâu đây\nthưa rằng ta ở đây đây đâu mà\nmột mình một bóng trăng vàng\nngười ta gặp gỡ một mình gặp nhau\ncớ sao gặp lại một người\nđể cho ai gặp một\n\n---\n","output_type":"stream"}]}]}